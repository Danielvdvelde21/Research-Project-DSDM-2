{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97455c2a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7253ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a48c33",
   "metadata": {},
   "source": [
    "# Scaling and Removing EventDay for one classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06149514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_models(df, seed=0):              \n",
    "    # Get rid of asthma attack days \n",
    "    df_withoutRowEventDay = df[df['EventDay'] == 0]\n",
    "    \n",
    "    # Remove now usless EventDay column\n",
    "    df_withoutColEventDay = df_withoutRowEventDay.drop(['EventDay'], axis = 1)\n",
    "\n",
    "    # Initialize the StandardScaler and scale the dataframe\n",
    "    scaler = StandardScaler()\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(df_withoutColEventDay), columns=df_withoutColEventDay.columns)\n",
    "    \n",
    "    '''\n",
    "    Important note!\n",
    "    Really important to use a random state here\n",
    "    Since the number of exacerbations is really low we should create multiple samples of the train test split\n",
    "    '''\n",
    "    \n",
    "    X_train, X_test = train_test_split(scaled_df, test_size=0.2, random_state=seed)\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7decc499",
   "metadata": {},
   "source": [
    "# Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4afb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(cm):\n",
    "    # Create a heatmap for visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def calculate_metrics(cm):\n",
    "    # Extracting values from confusion matrix\n",
    "    TP = cm[0][0]\n",
    "    TN = cm[1][1]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "\n",
    "    # Performance metrics\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return accuracy, precision, recall, specificity, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30ed56",
   "metadata": {},
   "source": [
    "# k-Means Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1314ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggested_no_of_clusters(min_clusters, max_clusters):\n",
    "    # Initialize lists to store the number of clusters and corresponding WCSS values\n",
    "    num_clusters = []\n",
    "    wcss_values = []\n",
    "\n",
    "    # Perform clustering for different numbers of clusters\n",
    "    for k in range(min_clusters, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(df)\n",
    "\n",
    "        # Compute the within-cluster sum of squares (WCSS)\n",
    "        wcss = kmeans.inertia_\n",
    "\n",
    "        # Append the number of clusters and WCSS values to the lists\n",
    "        num_clusters.append(k)\n",
    "        wcss_values.append(wcss)\n",
    "\n",
    "    # Calculate the differences between consecutive WCSS values\n",
    "    wcss_diff = [wcss_values[i] - wcss_values[i-1] for i in range(1, len(wcss_values))]\n",
    "\n",
    "    # Find the index of the maximum difference\n",
    "    max_diff_index = wcss_diff.index(max(wcss_diff))\n",
    "\n",
    "    # Suggested number of clusters\n",
    "    suggested_clusters = num_clusters[max_diff_index]\n",
    "\n",
    "    print(\"Suggested number of clusters:\", suggested_clusters)\n",
    "\n",
    "    \n",
    "# TODO can refine clusters for diffeent data sets\n",
    "# suggested_no_of_clusters(data_train, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea7b9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(df, data_train, data_pred, num_clusters=5, threshold=10):\n",
    "    # Select relevant columns for clustering\n",
    "    columns_to_cluster = data_train.columns\n",
    "\n",
    "    # Extract the subset of data for clustering from training DataFrame\n",
    "    data_for_clustering = data_train[columns_to_cluster]\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_for_clustering_standardized = scaler.fit_transform(data_for_clustering)\n",
    "\n",
    "    # Apply k-means clustering on training data\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(data_for_clustering_standardized)\n",
    "\n",
    "    # Get the cluster labels for training data\n",
    "    training_cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Assign cluster labels to the training DataFrame\n",
    "    data_train['ClusterLabel'] = training_cluster_labels\n",
    "\n",
    "    # Check the distribution of clusters in training data\n",
    "    training_cluster_counts = data_train['ClusterLabel'].value_counts()\n",
    "\n",
    "    # Calculate cluster means on training data\n",
    "    training_cluster_means = data_train.groupby('ClusterLabel').mean()\n",
    "\n",
    "    # Extract the subset of data for anomaly prediction from predicting DataFrame\n",
    "    data_for_prediction = data_pred[columns_to_cluster]\n",
    "\n",
    "    # Standardize the data for prediction using the same scaler as training\n",
    "    data_for_prediction_standardized = scaler.transform(data_for_prediction)\n",
    "\n",
    "    # Get the cluster labels for prediction data\n",
    "    prediction_cluster_labels = kmeans.predict(data_for_prediction_standardized)\n",
    "\n",
    "    # Assign cluster labels to the predicting DataFrame\n",
    "    data_pred['ClusterLabel'] = prediction_cluster_labels\n",
    "\n",
    "    # Check the distribution of clusters in predicting data\n",
    "    prediction_cluster_counts = data_pred['ClusterLabel'].value_counts()\n",
    "\n",
    "    # Detect anomalies based on cluster means\n",
    "    anomaly_rows = []\n",
    "    for index, row in data_pred.iterrows():\n",
    "        cluster_label = row['ClusterLabel']\n",
    "        features = row[columns_to_cluster]\n",
    "        cluster_mean = training_cluster_means.loc[cluster_label]\n",
    "        if any(abs(features - cluster_mean) > threshold):\n",
    "            anomaly_rows.append(index)\n",
    "\n",
    "    # detected anomalies\n",
    "    anomalies = data_pred.loc[anomaly_rows]\n",
    "    \n",
    "    df['Anomalies_KMeans'] = 0  # Initialize the column with 0 values\n",
    "    df.loc[anomalies.index, 'Anomalies_KMeans'] = 1  # Set the corresponding anomalies as 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ad95b",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0651b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iforest(df, data_train, data_pred, outliers_fraction=0.5):\n",
    "    # Train\n",
    "    ifo = IsolationForest(contamination = outliers_fraction)\n",
    "    ifo.fit(data_train)\n",
    "\n",
    "    # Predict\n",
    "    df['Anomalies_IF'] = pd.Series(ifo.predict(data_pred))\n",
    "    \n",
    "    # Configure properly\n",
    "    df['Anomalies_IF'].fillna(0, inplace=True)\n",
    "    df['Anomalies_IF'] = df['Anomalies_IF'].replace(1, 0)\n",
    "    df['Anomalies_IF'] = df['Anomalies_IF'].replace(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ba10e",
   "metadata": {},
   "source": [
    "# OSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1d1d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocsvm(df, data_train, data_pred, outliers_fraction=0.9):\n",
    "    # Train\n",
    "    osvm = OneClassSVM(nu = outliers_fraction, kernel = 'sigmoid')\n",
    "    osvm.fit(data_train)\n",
    "\n",
    "    # Predict\n",
    "    df['Anomalies_OSVM'] = pd.Series(osvm.predict(data_pred))\n",
    "    \n",
    "    # Configure properly\n",
    "    df['Anomalies_OSVM'].fillna(0, inplace=True)\n",
    "    df['Anomalies_OSVM'] = df['Anomalies_OSVM'].replace(1, 0)\n",
    "    df['Anomalies_OSVM'] = df['Anomalies_OSVM'].replace(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48e78b",
   "metadata": {},
   "source": [
    "# Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eb86b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_of(df, data_train, data_pred, outliers_fraction=0.3):\n",
    "    # Train\n",
    "    lof = LocalOutlierFactor(contamination=outliers_fraction)\n",
    "    lof.fit(data_train)\n",
    "\n",
    "    # Predict\n",
    "    df['Anomalies_LOF'] = pd.Series(lof.fit_predict(data_pred))\n",
    "    \n",
    "    # Configure properly\n",
    "    df['Anomalies_LOF'].fillna(0, inplace=True)\n",
    "    df['Anomalies_LOF'] = df['Anomalies_LOF'].replace(1, 0)\n",
    "    df['Anomalies_LOF'] = df['Anomalies_LOF'].replace(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359f12d",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b5c3dc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def autoencoders(df, train, test):\n",
    "    # Define the shape of the input data\n",
    "    input_dim = train.shape[1]\n",
    "\n",
    "    # Define the architecture of the autoencoder\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(32, activation='relu')(input_layer)  # Encoding layer\n",
    "    decoder = Dense(input_dim, activation='linear')(encoder)  # Decoding layer\n",
    "\n",
    "    # Create the autoencoder model\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(train, train, epochs=50, batch_size=32, validation_data=(test, test), verbose=0)\n",
    "\n",
    "    # Use the trained autoencoder to predict on the test dataset\n",
    "    reconstructed_data = autoencoder.predict(test)\n",
    "\n",
    "    # Calculate the mean squared error (MSE) between the original and reconstructed data\n",
    "    mse = np.mean(np.power(test - reconstructed_data, 2), axis=1)\n",
    "\n",
    "    # Define a threshold to determine outliers\n",
    "    threshold = np.mean(mse) + 3 * np.std(mse)  # Adjust the multiplier (3) as per your requirements\n",
    "\n",
    "    # Map outliers as -1 and inliers as 1\n",
    "    outliers = np.where(mse > threshold, 1, 0)\n",
    "\n",
    "    # Return anomalies\n",
    "    anomalies = test.copy()\n",
    "    anomalies['Anomalies_AE'] = outliers    \n",
    "    \n",
    "    # Add the anomaly column to the original data\n",
    "    df['Anomalies_AE'] = 0  # Initialize all values as 1 (normal)\n",
    "    df.iloc[test.index, -1] = anomalies['Anomalies_AE'].values  # Assign anomaly values to test data indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025298cb",
   "metadata": {},
   "source": [
    "# Deep Support Vector Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d6d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsvdd(df, train, test):\n",
    "    # Train the autoencoder\n",
    "    input_dim = train.shape[1]\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    encoder = Dense(32, activation='relu')(input_layer)  # Adjust the number of nodes in the encoder layer\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    autoencoder.fit(train, train, epochs=50, batch_size=32, shuffle=True, validation_data=(test, test), verbose=0)\n",
    "\n",
    "    # Obtain the encoded representations from the trained autoencoder\n",
    "    encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "    encoded_train_data = encoder_model.predict(train)\n",
    "    encoded_test_data = encoder_model.predict(test)\n",
    "\n",
    "    # Train the ODSVDDM using the encoded representations\n",
    "    deep_svdd = DeepSVDD(verbose=0)\n",
    "    deep_svdd.fit(encoded_train_data)\n",
    "\n",
    "    # Predict the anomaly scores for the test data\n",
    "    anomaly_scores = deep_svdd.decision_function(encoded_test_data)\n",
    "\n",
    "    # Calculate the threshold for anomaly detection using the anomaly scores\n",
    "    threshold = np.percentile(anomaly_scores, 97)  # Adjust the percentile as needed\n",
    "\n",
    "    # Classify data points as normal (inliers) or anomalous (outliers)\n",
    "    predictions = np.where(anomaly_scores > threshold, 1, 0)\n",
    "\n",
    "    # Add the anomaly column to the original data\n",
    "    df['Anomalies_DSVDD'] = 0  # Initialize all values as 1 (inliers)\n",
    "    df.iloc[test.index, -1] = predictions  # Assign anomaly values to test data indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad6e98",
   "metadata": {},
   "source": [
    "# Load any preprocessed File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1aa9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move up 2 directories\n",
    "data_directory = '../..' \n",
    "\n",
    "# Load the CSV files\n",
    "asthma_df = pd.read_csv(os.path.join(data_directory, 'Data\\Preprocessed', 'preprocessed_MICE_asthma.csv'))\n",
    "healthy_df = pd.read_csv(os.path.join(data_directory, 'Data\\Preprocessed', 'preprocessed_MICE_healthy.csv'))\n",
    "\n",
    "# Merged df\n",
    "merged_df = pd.concat([asthma_df, healthy_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81d66b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DayNo</th>\n",
       "      <th>Age</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>BMI_SDS</th>\n",
       "      <th>PedsQL_score_baseline</th>\n",
       "      <th>stepsTotalDaily</th>\n",
       "      <th>steps_hour_max</th>\n",
       "      <th>steps07</th>\n",
       "      <th>steps08</th>\n",
       "      <th>steps09</th>\n",
       "      <th>steps10</th>\n",
       "      <th>steps11</th>\n",
       "      <th>steps12</th>\n",
       "      <th>steps13</th>\n",
       "      <th>steps14</th>\n",
       "      <th>steps15</th>\n",
       "      <th>steps16</th>\n",
       "      <th>steps17</th>\n",
       "      <th>steps18</th>\n",
       "      <th>steps19</th>\n",
       "      <th>steps20</th>\n",
       "      <th>HR05Perc</th>\n",
       "      <th>HR95Perc</th>\n",
       "      <th>HRMinSleep</th>\n",
       "      <th>HRMaxSleep</th>\n",
       "      <th>AVGHR_daily</th>\n",
       "      <th>AVGHR_sleep</th>\n",
       "      <th>AVGHR_wake</th>\n",
       "      <th>HR00</th>\n",
       "      <th>HR01</th>\n",
       "      <th>HR02</th>\n",
       "      <th>HR03</th>\n",
       "      <th>HR04</th>\n",
       "      <th>HR05</th>\n",
       "      <th>HR06</th>\n",
       "      <th>HR07</th>\n",
       "      <th>HR08</th>\n",
       "      <th>HR09</th>\n",
       "      <th>HR10</th>\n",
       "      <th>HR11</th>\n",
       "      <th>HR12</th>\n",
       "      <th>HR13</th>\n",
       "      <th>HR14</th>\n",
       "      <th>HR15</th>\n",
       "      <th>HR16</th>\n",
       "      <th>HR17</th>\n",
       "      <th>HR18</th>\n",
       "      <th>HR19</th>\n",
       "      <th>HR20</th>\n",
       "      <th>HR21</th>\n",
       "      <th>HR22</th>\n",
       "      <th>HR23</th>\n",
       "      <th>wear05H</th>\n",
       "      <th>wear16H</th>\n",
       "      <th>wear24H</th>\n",
       "      <th>awakeDuration</th>\n",
       "      <th>lightSleepDuration</th>\n",
       "      <th>deepSleepDuration</th>\n",
       "      <th>wakeUpCount</th>\n",
       "      <th>FG</th>\n",
       "      <th>FHX</th>\n",
       "      <th>FHN</th>\n",
       "      <th>TG</th>\n",
       "      <th>TN</th>\n",
       "      <th>TX</th>\n",
       "      <th>SQ</th>\n",
       "      <th>SP</th>\n",
       "      <th>DR</th>\n",
       "      <th>RH</th>\n",
       "      <th>RHX</th>\n",
       "      <th>EventDay</th>\n",
       "      <th>weekday_Fri</th>\n",
       "      <th>weekday_Mon</th>\n",
       "      <th>weekday_Sat</th>\n",
       "      <th>weekday_Sun</th>\n",
       "      <th>weekday_Thu</th>\n",
       "      <th>weekday_Tue</th>\n",
       "      <th>weekday_Wed</th>\n",
       "      <th>dayType_holiday</th>\n",
       "      <th>dayType_school</th>\n",
       "      <th>dayType_weekend</th>\n",
       "      <th>school_yes_no_no</th>\n",
       "      <th>school_yes_no_yes</th>\n",
       "      <th>sex_Female</th>\n",
       "      <th>sex_Male</th>\n",
       "      <th>sportsyesno_No</th>\n",
       "      <th>sportsyesno_Yes</th>\n",
       "      <th>urbanisation_Extremely urbanised</th>\n",
       "      <th>urbanisation_Not extremely urbanised</th>\n",
       "      <th>grade_fev1_A</th>\n",
       "      <th>grade_fev1_B</th>\n",
       "      <th>grade_fev1_C</th>\n",
       "      <th>grade_fev1_D</th>\n",
       "      <th>grade_fev1_E</th>\n",
       "      <th>grade_fev1_F</th>\n",
       "      <th>grade_fev1_U</th>\n",
       "      <th>grade_fvc_A</th>\n",
       "      <th>grade_fvc_B</th>\n",
       "      <th>grade_fvc_C</th>\n",
       "      <th>grade_fvc_D</th>\n",
       "      <th>grade_fvc_E</th>\n",
       "      <th>grade_fvc_F</th>\n",
       "      <th>grade_fvc_U</th>\n",
       "      <th>screentime_0-30 min</th>\n",
       "      <th>screentime_0.5-1 hours</th>\n",
       "      <th>screentime_1-2 hours</th>\n",
       "      <th>screentime_2-4 hours</th>\n",
       "      <th>screentime_&gt; 4 hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>51.9</td>\n",
       "      <td>163.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>847826087.0</td>\n",
       "      <td>3723.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>147.660462</td>\n",
       "      <td>198.589585</td>\n",
       "      <td>63.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>449.061499</td>\n",
       "      <td>205.326828</td>\n",
       "      <td>605.0</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>514.09455</td>\n",
       "      <td>82.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>75.312548</td>\n",
       "      <td>94.0</td>\n",
       "      <td>72.354658</td>\n",
       "      <td>70.32749</td>\n",
       "      <td>72.162935</td>\n",
       "      <td>72.645209</td>\n",
       "      <td>70.346684</td>\n",
       "      <td>70.987121</td>\n",
       "      <td>73.239849</td>\n",
       "      <td>85.81755</td>\n",
       "      <td>94.229593</td>\n",
       "      <td>91.550898</td>\n",
       "      <td>93.735671</td>\n",
       "      <td>120.0</td>\n",
       "      <td>96.217727</td>\n",
       "      <td>93.848718</td>\n",
       "      <td>102.29268</td>\n",
       "      <td>95.356064</td>\n",
       "      <td>94.624175</td>\n",
       "      <td>89.8</td>\n",
       "      <td>83.4</td>\n",
       "      <td>87.8</td>\n",
       "      <td>91.5</td>\n",
       "      <td>93.30</td>\n",
       "      <td>77.1</td>\n",
       "      <td>78.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>16800.0</td>\n",
       "      <td>16440.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>5.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>51.9</td>\n",
       "      <td>163.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>847826087.0</td>\n",
       "      <td>10015.0</td>\n",
       "      <td>4355.0</td>\n",
       "      <td>41.399402</td>\n",
       "      <td>607.000000</td>\n",
       "      <td>580.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>181.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>161.00000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>744.0</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>4355.0</td>\n",
       "      <td>722.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>102.0</td>\n",
       "      <td>73.600000</td>\n",
       "      <td>75.20000</td>\n",
       "      <td>70.750000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>70.300000</td>\n",
       "      <td>82.300000</td>\n",
       "      <td>78.00000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>129.800000</td>\n",
       "      <td>95.1</td>\n",
       "      <td>91.600000</td>\n",
       "      <td>105.358749</td>\n",
       "      <td>108.50000</td>\n",
       "      <td>85.250000</td>\n",
       "      <td>93.400000</td>\n",
       "      <td>95.8</td>\n",
       "      <td>101.8</td>\n",
       "      <td>142.3</td>\n",
       "      <td>107.8</td>\n",
       "      <td>104.20</td>\n",
       "      <td>91.8</td>\n",
       "      <td>93.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>22200.0</td>\n",
       "      <td>11760.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>51.9</td>\n",
       "      <td>163.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>847826087.0</td>\n",
       "      <td>3811.0</td>\n",
       "      <td>727.0</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>159.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>591.000000</td>\n",
       "      <td>109.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>456.00000</td>\n",
       "      <td>155.0</td>\n",
       "      <td>727.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>96.0</td>\n",
       "      <td>79.100000</td>\n",
       "      <td>73.10000</td>\n",
       "      <td>80.300000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>76.100000</td>\n",
       "      <td>74.600000</td>\n",
       "      <td>78.100000</td>\n",
       "      <td>98.50000</td>\n",
       "      <td>103.400000</td>\n",
       "      <td>100.500000</td>\n",
       "      <td>109.600000</td>\n",
       "      <td>100.6</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>86.50000</td>\n",
       "      <td>88.600000</td>\n",
       "      <td>102.800000</td>\n",
       "      <td>78.2</td>\n",
       "      <td>85.2</td>\n",
       "      <td>94.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>89.25</td>\n",
       "      <td>96.5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>19260.0</td>\n",
       "      <td>12360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>62.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>8.2</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>51.9</td>\n",
       "      <td>163.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>847826087.0</td>\n",
       "      <td>4346.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>119.919027</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>673.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>322.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>523.00000</td>\n",
       "      <td>61.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>93.0</td>\n",
       "      <td>77.100000</td>\n",
       "      <td>79.80000</td>\n",
       "      <td>72.600000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>80.200000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>73.600000</td>\n",
       "      <td>79.00000</td>\n",
       "      <td>98.500000</td>\n",
       "      <td>103.500000</td>\n",
       "      <td>106.400000</td>\n",
       "      <td>95.5</td>\n",
       "      <td>106.200000</td>\n",
       "      <td>98.200000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>84.300000</td>\n",
       "      <td>86.800000</td>\n",
       "      <td>97.2</td>\n",
       "      <td>74.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>82.4</td>\n",
       "      <td>96.10</td>\n",
       "      <td>90.8</td>\n",
       "      <td>78.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>17041.0</td>\n",
       "      <td>19619.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>63.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>51.9</td>\n",
       "      <td>163.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>847826087.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>1324.0</td>\n",
       "      <td>27.834972</td>\n",
       "      <td>223.003456</td>\n",
       "      <td>50.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>1324.000000</td>\n",
       "      <td>388.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>101.00000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83.300000</td>\n",
       "      <td>77.60000</td>\n",
       "      <td>75.100000</td>\n",
       "      <td>76.200000</td>\n",
       "      <td>72.600000</td>\n",
       "      <td>75.800000</td>\n",
       "      <td>73.800000</td>\n",
       "      <td>68.00000</td>\n",
       "      <td>74.800000</td>\n",
       "      <td>65.400000</td>\n",
       "      <td>94.400000</td>\n",
       "      <td>85.7</td>\n",
       "      <td>77.250000</td>\n",
       "      <td>86.500000</td>\n",
       "      <td>78.00000</td>\n",
       "      <td>76.600000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>89.1</td>\n",
       "      <td>92.2</td>\n",
       "      <td>110.6</td>\n",
       "      <td>77.4</td>\n",
       "      <td>81.10</td>\n",
       "      <td>76.5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>17340.0</td>\n",
       "      <td>15300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>98.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DayNo   Age  weight  height  BMI_SDS  PedsQL_score_baseline  \\\n",
       "0    0.0  15.0    51.9   163.2     -0.1            847826087.0   \n",
       "1    1.0  15.0    51.9   163.2     -0.1            847826087.0   \n",
       "2    2.0  15.0    51.9   163.2     -0.1            847826087.0   \n",
       "3    3.0  15.0    51.9   163.2     -0.1            847826087.0   \n",
       "4    4.0  15.0    51.9   163.2     -0.1            847826087.0   \n",
       "\n",
       "   stepsTotalDaily  steps_hour_max     steps07     steps08  steps09  steps10  \\\n",
       "0           3723.0          1640.0  147.660462  198.589585     63.0    137.0   \n",
       "1          10015.0          4355.0   41.399402  607.000000    580.0    325.0   \n",
       "2           3811.0           727.0   63.000000  561.000000    159.0    356.0   \n",
       "3           4346.0           673.0  119.919027  515.000000    673.0    377.0   \n",
       "4           3270.0          1324.0   27.834972  223.003456     50.0     92.0   \n",
       "\n",
       "      steps11      steps12  steps13  steps14    steps15  steps16  steps17  \\\n",
       "0  449.061499   205.326828    605.0   1640.0  514.09455     82.0    451.0   \n",
       "1  180.000000   322.000000    181.0    491.0  161.00000     10.0    744.0   \n",
       "2   26.000000   591.000000    109.0    114.0  456.00000    155.0    727.0   \n",
       "3  490.000000   192.000000    322.0    180.0  523.00000     61.0    359.0   \n",
       "4  167.000000  1324.000000    388.0    166.0  101.00000     91.0      6.0   \n",
       "\n",
       "   steps18  steps19  steps20  HR05Perc  HR95Perc  HRMinSleep  HRMaxSleep  \\\n",
       "0    245.0    135.0    160.0      68.0     120.0        61.0        93.0   \n",
       "1   1146.0   4355.0    722.0      82.0     168.0        67.0       121.0   \n",
       "2     46.0    139.0    210.0      71.0     122.0        64.0       129.0   \n",
       "3    174.0     95.0    148.0      66.0     128.0        59.0       107.0   \n",
       "4    224.0    405.0     56.0      64.0     111.0        57.0        88.0   \n",
       "\n",
       "   AVGHR_daily  AVGHR_sleep  AVGHR_wake       HR00      HR01       HR02  \\\n",
       "0         90.0    75.312548        94.0  72.354658  70.32749  72.162935   \n",
       "1         94.0    76.000000       102.0  73.600000  75.20000  70.750000   \n",
       "2         90.0    77.000000        96.0  79.100000  73.10000  80.300000   \n",
       "3         88.0    75.000000        93.0  77.100000  79.80000  72.600000   \n",
       "4         81.0    77.000000        83.0  83.300000  77.60000  75.100000   \n",
       "\n",
       "        HR03       HR04       HR05       HR06      HR07        HR08  \\\n",
       "0  72.645209  70.346684  70.987121  73.239849  85.81755   94.229593   \n",
       "1  92.000000  76.000000  70.300000  82.300000  78.00000   99.000000   \n",
       "2  78.000000  76.100000  74.600000  78.100000  98.50000  103.400000   \n",
       "3  67.000000  80.200000  76.000000  73.600000  79.00000   98.500000   \n",
       "4  76.200000  72.600000  75.800000  73.800000  68.00000   74.800000   \n",
       "\n",
       "         HR09        HR10   HR11        HR12        HR13       HR14  \\\n",
       "0   91.550898   93.735671  120.0   96.217727   93.848718  102.29268   \n",
       "1  113.000000  129.800000   95.1   91.600000  105.358749  108.50000   \n",
       "2  100.500000  109.600000  100.6  105.000000  109.000000   86.50000   \n",
       "3  103.500000  106.400000   95.5  106.200000   98.200000  100.00000   \n",
       "4   65.400000   94.400000   85.7   77.250000   86.500000   78.00000   \n",
       "\n",
       "        HR15        HR16  HR17   HR18   HR19   HR20    HR21  HR22  HR23  \\\n",
       "0  95.356064   94.624175  89.8   83.4   87.8   91.5   93.30  77.1  78.6   \n",
       "1  85.250000   93.400000  95.8  101.8  142.3  107.8  104.20  91.8  93.0   \n",
       "2  88.600000  102.800000  78.2   85.2   94.0  100.0   89.25  96.5  82.0   \n",
       "3  84.300000   86.800000  97.2   74.0  107.0   82.4   96.10  90.8  78.2   \n",
       "4  76.600000   90.000000  89.1   92.2  110.6   77.4   81.10  76.5  82.0   \n",
       "\n",
       "   wear05H  wear16H  wear24H  awakeDuration  lightSleepDuration  \\\n",
       "0      0.0     69.0     54.0          660.0             16800.0   \n",
       "1    100.0    100.0    100.0          180.0             22200.0   \n",
       "2    100.0    100.0    100.0          300.0             19260.0   \n",
       "3    100.0    100.0    100.0         1140.0             17041.0   \n",
       "4    100.0    100.0    100.0          540.0             17340.0   \n",
       "\n",
       "   deepSleepDuration  wakeUpCount    FG   FHX  FHN    TG     TN    TX   SQ  \\\n",
       "0            16440.0          1.0   6.3   8.0  4.0  14.0   85.0  19.7  5.4   \n",
       "1            11760.0          0.0  10.0  12.0  8.0  11.3  100.0  13.1  0.2   \n",
       "2            12360.0          1.0   6.0   9.0  4.0   9.9   62.0  13.3  8.2   \n",
       "3            19619.0          1.0   6.9  10.0  4.0   9.5   63.0  11.7  3.2   \n",
       "4            15300.0          2.0   9.3  12.0  6.0  11.2   98.0  12.9  0.5   \n",
       "\n",
       "     SP   DR    RH   RHX  EventDay  weekday_Fri  weekday_Mon  weekday_Sat  \\\n",
       "0  58.0  0.0   0.0   0.0       0.0          0.0          0.0          0.0   \n",
       "1   2.0  0.9   2.0   1.0       0.0          0.0          0.0          0.0   \n",
       "2  89.0  0.0   0.0   0.0       0.0          0.0          0.0          0.0   \n",
       "3  35.0  0.0   0.0   0.0       0.0          1.0          0.0          0.0   \n",
       "4   5.0  5.5  38.0  10.0       0.0          0.0          0.0          1.0   \n",
       "\n",
       "   weekday_Sun  weekday_Thu  weekday_Tue  weekday_Wed  dayType_holiday  \\\n",
       "0          0.0          0.0          1.0          0.0              0.0   \n",
       "1          0.0          0.0          0.0          1.0              0.0   \n",
       "2          0.0          1.0          0.0          0.0              0.0   \n",
       "3          0.0          0.0          0.0          0.0              0.0   \n",
       "4          0.0          0.0          0.0          0.0              0.0   \n",
       "\n",
       "   dayType_school  dayType_weekend  school_yes_no_no  school_yes_no_yes  \\\n",
       "0             1.0              0.0               0.0                0.0   \n",
       "1             1.0              0.0               0.0                1.0   \n",
       "2             1.0              0.0               0.0                1.0   \n",
       "3             1.0              0.0               0.0                1.0   \n",
       "4             0.0              1.0               1.0                0.0   \n",
       "\n",
       "   sex_Female  sex_Male  sportsyesno_No  sportsyesno_Yes  \\\n",
       "0         1.0       0.0             0.0              1.0   \n",
       "1         1.0       0.0             0.0              1.0   \n",
       "2         1.0       0.0             0.0              1.0   \n",
       "3         1.0       0.0             0.0              1.0   \n",
       "4         1.0       0.0             0.0              1.0   \n",
       "\n",
       "   urbanisation_Extremely urbanised  urbanisation_Not extremely urbanised  \\\n",
       "0                               1.0                                   0.0   \n",
       "1                               1.0                                   0.0   \n",
       "2                               1.0                                   0.0   \n",
       "3                               1.0                                   0.0   \n",
       "4                               1.0                                   0.0   \n",
       "\n",
       "   grade_fev1_A  grade_fev1_B  grade_fev1_C  grade_fev1_D  grade_fev1_E  \\\n",
       "0           0.0           0.0           0.0           1.0           0.0   \n",
       "1           1.0           0.0           0.0           0.0           0.0   \n",
       "2           0.0           1.0           0.0           0.0           0.0   \n",
       "3           0.0           1.0           0.0           0.0           0.0   \n",
       "4           0.0           1.0           0.0           0.0           0.0   \n",
       "\n",
       "   grade_fev1_F  grade_fev1_U  grade_fvc_A  grade_fvc_B  grade_fvc_C  \\\n",
       "0           0.0           0.0          0.0          1.0          0.0   \n",
       "1           0.0           0.0          1.0          0.0          0.0   \n",
       "2           0.0           0.0          0.0          1.0          0.0   \n",
       "3           0.0           0.0          0.0          1.0          0.0   \n",
       "4           0.0           0.0          0.0          1.0          0.0   \n",
       "\n",
       "   grade_fvc_D  grade_fvc_E  grade_fvc_F  grade_fvc_U  screentime_0-30 min  \\\n",
       "0          0.0          0.0          0.0          0.0                  0.0   \n",
       "1          0.0          0.0          0.0          0.0                  0.0   \n",
       "2          0.0          0.0          0.0          0.0                  0.0   \n",
       "3          0.0          0.0          0.0          0.0                  0.0   \n",
       "4          0.0          0.0          0.0          0.0                  0.0   \n",
       "\n",
       "   screentime_0.5-1 hours  screentime_1-2 hours  screentime_2-4 hours  \\\n",
       "0                     0.0                   0.0                   0.0   \n",
       "1                     1.0                   0.0                   0.0   \n",
       "2                     1.0                   0.0                   0.0   \n",
       "3                     1.0                   0.0                   0.0   \n",
       "4                     1.0                   0.0                   0.0   \n",
       "\n",
       "   screentime_> 4 hours  \n",
       "0                   0.0  \n",
       "1                   0.0  \n",
       "2                   0.0  \n",
       "3                   0.0  \n",
       "4                   0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cda02",
   "metadata": {},
   "source": [
    "# Preprocess data for models (Removing eventdate and scaling the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc9a011",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 1ms/step\n",
      "160/160 [==============================] - 0s 902us/step\n",
      "40/40 [==============================] - 0s 1ms/step\n",
      "160/160 [==============================] - 0s 848us/step\n",
      "160/160 [==============================] - 0s 1ms/step\n",
      "40/40 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Preprocess for models\n",
    "train, pred = preprocess_for_models(merged_df)\n",
    "\n",
    "# Run all models and store anomaly results in merged_df\n",
    "k_means(merged_df, train, pred)\n",
    "iforest(merged_df, train, pred)\n",
    "local_of(merged_df, train, pred)\n",
    "ocsvm(merged_df, train, pred)\n",
    "autoencoders(merged_df, train, pred) \n",
    "dsvdd(merged_df, train, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd039ed",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Obtain performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dddf1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies_KMeans\n",
      "Accuracy: 0.98125 \n",
      "Precision: 0.9897557131599685 \n",
      "Recall: 0.9913180741910024 \n",
      "Specificity: 0.0 \n",
      "F1 Score: 0.9905362776025236\n",
      "\n",
      "Anomalies_IF\n",
      "Accuracy: 0.9 \n",
      "Precision: 0.9905254091300603 \n",
      "Recall: 0.9076558800315706 \n",
      "Specificity: 0.15384615384615385 \n",
      "F1 Score: 0.9472817133443163\n",
      "\n",
      "Anomalies_OSVM\n",
      "Accuracy: 0.8234375 \n",
      "Precision: 0.9924314096499527 \n",
      "Recall: 0.8279400157853196 \n",
      "Specificity: 0.38461538461538464 \n",
      "F1 Score: 0.9027538726333907\n",
      "\n",
      "Anomalies_LOF\n",
      "Accuracy: 0.940625 \n",
      "Precision: 0.9909315746084089 \n",
      "Recall: 0.9486977111286503 \n",
      "Specificity: 0.15384615384615385 \n",
      "F1 Score: 0.9693548387096774\n",
      "\n",
      "Anomalies_AE\n",
      "Accuracy: 0.96796875 \n",
      "Precision: 0.9896166134185304 \n",
      "Recall: 0.9779005524861878 \n",
      "Specificity: 0.0 \n",
      "F1 Score: 0.9837236998809051\n",
      "\n",
      "Anomalies_DSVDD\n",
      "Accuracy: 0.9609375 \n",
      "Precision: 0.9903303787268332 \n",
      "Recall: 0.9700078926598263 \n",
      "Specificity: 0.07692307692307693 \n",
      "F1 Score: 0.9800637958532695\n"
     ]
    }
   ],
   "source": [
    "# Only use the test dataset to calculate performance metrics\n",
    "subset_df = merged_df.loc[pred.index]\n",
    "\n",
    "print(\"Anomalies_KMeans\")\n",
    "cm = confusion_matrix(subset_df['Anomalies_KMeans'].values, subset_df['EventDay'].values)\n",
    "display_confusion_matrix(cm)\n",
    "accuracy, precision, recall, specificity, f1_score = calculate_metrics(cm)\n",
    "print(\"Accuracy:\", accuracy, \"\\nPrecision:\", precision, \"\\nRecall:\", recall, \n",
    "      \"\\nSpecificity:\", specificity, \"\\nF1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47732d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anomalies_IF\")\n",
    "cm = confusion_matrix(subset_df['Anomalies_IF'].values, subset_df['EventDay'].values)\n",
    "display_confusion_matrix(cm)\n",
    "accuracy, precision, recall, specificity, f1_score = calculate_metrics(cm)\n",
    "print(\"Accuracy:\", accuracy, \"\\nPrecision:\", precision, \"\\nRecall:\", recall, \n",
    "      \"\\nSpecificity:\", specificity, \"\\nF1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb50390",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anomalies_OSVM\")\n",
    "cm = confusion_matrix(subset_df['Anomalies_OSVM'].values, subset_df['EventDay'].values)\n",
    "display_confusion_matrix(cm)\n",
    "accuracy, precision, recall, specificity, f1_score = calculate_metrics(cm)\n",
    "print(\"Accuracy:\", accuracy, \"\\nPrecision:\", precision, \"\\nRecall:\", recall, \n",
    "      \"\\nSpecificity:\", specificity, \"\\nF1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f307b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anomalies_LOF\")\n",
    "cm = confusion_matrix(subset_df['Anomalies_LOF'].values, subset_df['EventDay'].values)\n",
    "display_confusion_matrix(cm)\n",
    "accuracy, precision, recall, specificity, f1_score = calculate_metrics(cm)\n",
    "print(\"Accuracy:\", accuracy, \"\\nPrecision:\", precision, \"\\nRecall:\", recall, \n",
    "      \"\\nSpecificity:\", specificity, \"\\nF1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31616b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anomalies_AE\")\n",
    "cm = confusion_matrix(subset_df['Anomalies_AE'].values, subset_df['EventDay'].values)\n",
    "display_confusion_matrix(cm)\n",
    "accuracy, precision, recall, specificity, f1_score = calculate_metrics(cm)\n",
    "print(\"Accuracy:\", accuracy, \"\\nPrecision:\", precision, \"\\nRecall:\", recall, \n",
    "      \"\\nSpecificity:\", specificity, \"\\nF1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d32bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anomalies_DSVDD\")\n",
    "cm = confusion_matrix(subset_df['Anomalies_DSVDD'].values, subset_df['EventDay'].values)\n",
    "display_confusion_matrix(cm)\n",
    "accuracy, precision, recall, specificity, f1_score = calculate_metrics(cm)\n",
    "print(\"Accuracy:\", accuracy, \"\\nPrecision:\", precision, \"\\nRecall:\", recall, \n",
    "      \"\\nSpecificity:\", specificity, \"\\nF1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a287c9",
   "metadata": {},
   "source": [
    "# Best configuration of train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f66a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 957us/step\n",
      "160/160 [==============================] - 0s 774us/step\n",
      "40/40 [==============================] - 0s 919us/step\n",
      "160/160 [==============================] - 0s 833us/step\n",
      "160/160 [==============================] - 0s 939us/step\n",
      "40/40 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                               | 1/100 [01:15<2:04:10, 75.26s/it]\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 100\n",
    "best_seed = None\n",
    "best_f1_score = 0.0\n",
    "\n",
    "# Perform train-test split for different random seeds\n",
    "# Use the best seed as default train test split\n",
    "for i in tqdm(range(num_iterations)):\n",
    "    seed = i  # Set the seed for each iteration\n",
    "\n",
    "    # Preprocess for models\n",
    "    merged_df = pd.concat([asthma_df, healthy_df], ignore_index=True)\n",
    "    \n",
    "    # Create train test split\n",
    "    train, test = preprocess_for_models(merged_df, seed)\n",
    "    \n",
    "    # Run all models and store anomaly results in merged_df\n",
    "    k_means(merged_df, train, test)\n",
    "    iforest(merged_df, train, test)\n",
    "    local_of(merged_df, train, test)\n",
    "    ocsvm(merged_df, train, test)\n",
    "    autoencoders(merged_df, train, test) \n",
    "    dsvdd(merged_df, train, test)\n",
    "    \n",
    "    # Get all f1 scores and average the number\n",
    "    subset_df = merged_df.loc[test.index]\n",
    "    cm = confusion_matrix(subset_df['Anomalies_KMeans'].values, subset_df['EventDay'].values)\n",
    "    _, _, _, _, f1 = calculate_metrics(cm)\n",
    "    \n",
    "    cm = confusion_matrix(subset_df['Anomalies_IF'].values, subset_df['EventDay'].values)\n",
    "    _, _, _, _, f2 = calculate_metrics(cm)\n",
    "    \n",
    "    cm = confusion_matrix(subset_df['Anomalies_OSVM'].values, subset_df['EventDay'].values)\n",
    "    _, _, _, _, f3 = calculate_metrics(cm)\n",
    "    \n",
    "    cm = confusion_matrix(subset_df['Anomalies_LOF'].values, subset_df['EventDay'].values)\n",
    "    _, _, _, _, f4 = calculate_metrics(cm)\n",
    "    \n",
    "    cm = confusion_matrix(subset_df['Anomalies_AE'].values, subset_df['EventDay'].values)\n",
    "    _, _, _, _, f5 = calculate_metrics(cm)\n",
    "    \n",
    "    cm = confusion_matrix(subset_df['Anomalies_DSVDD'].values, subset_df['EventDay'].values)\n",
    "    _, _, _, _, f6 = calculate_metrics(cm)\n",
    "    \n",
    "    f_score = (f1 + f2 + f3 + f4 + f5 + f6) / 6 \n",
    "    \n",
    "    # Update the best F1 score and seed if necessary\n",
    "    if f_score > best_f1_score:\n",
    "        best_f1_score = f_score\n",
    "        best_seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2125181",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Seed:\", best_seed)\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "\n",
    "train.to_csv('optimal_train.csv', index=False)\n",
    "test.to_csv('optimal_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e2c0cb",
   "metadata": {},
   "source": [
    "# TODO FEATURE SELECTION\n",
    "# TODO TWEAK PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ea14a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
