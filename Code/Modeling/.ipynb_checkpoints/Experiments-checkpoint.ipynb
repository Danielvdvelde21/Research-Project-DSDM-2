{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97455c2a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a48c33",
   "metadata": {},
   "source": [
    "# Scaling and Removing EventDay for one classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06149514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_models(df):              \n",
    "    # Get rid of asthma attack days \n",
    "    df_withoutRowEventDay = df[df['EventDay'] == 0]\n",
    "    \n",
    "    # Remove now usless EventDay column\n",
    "    df_withoutColEventDay = df_withoutRowEventDay.drop(['EventDay'], axis = 1)\n",
    "\n",
    "    # Initialize the StandardScaler and scale the dataframe\n",
    "    scaler = StandardScaler()\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(df_withoutColEventDay), columns=df_withoutColEventDay.columns)\n",
    "    \n",
    "    '''\n",
    "    Important note!\n",
    "    Really important to use a random state here\n",
    "    Since the number of exacerbations is really low we should create multiple samples of the train test split\n",
    "    '''\n",
    "    \n",
    "    X_train, X_test = train_test_split(scaled_df, test_size=0.2)\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7decc499",
   "metadata": {},
   "source": [
    "# Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4afb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(cm):\n",
    "    # Create a heatmap for visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def calculate_metrics(cm):\n",
    "    # Extracting values from confusion matrix\n",
    "    TP = cm[0][0]\n",
    "    TN = cm[1][1]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "\n",
    "    # Calculating accuracy\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "    # Calculating precision\n",
    "    precision = TP / (TP + FP)\n",
    "\n",
    "    # Calculating recall\n",
    "    recall = TP / (TP + FN)\n",
    "\n",
    "    # Calculating F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30ed56",
   "metadata": {},
   "source": [
    "# k-Means Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1314ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggested_no_of_clusters(min_clusters, max_clusters):\n",
    "    # Initialize lists to store the number of clusters and corresponding WCSS values\n",
    "    num_clusters = []\n",
    "    wcss_values = []\n",
    "\n",
    "    # Perform clustering for different numbers of clusters\n",
    "    for k in range(min_clusters, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(df)\n",
    "\n",
    "        # Compute the within-cluster sum of squares (WCSS)\n",
    "        wcss = kmeans.inertia_\n",
    "\n",
    "        # Append the number of clusters and WCSS values to the lists\n",
    "        num_clusters.append(k)\n",
    "        wcss_values.append(wcss)\n",
    "\n",
    "    # Calculate the differences between consecutive WCSS values\n",
    "    wcss_diff = [wcss_values[i] - wcss_values[i-1] for i in range(1, len(wcss_values))]\n",
    "\n",
    "    # Find the index of the maximum difference\n",
    "    max_diff_index = wcss_diff.index(max(wcss_diff))\n",
    "\n",
    "    # Suggested number of clusters\n",
    "    suggested_clusters = num_clusters[max_diff_index]\n",
    "\n",
    "    print(\"Suggested number of clusters:\", suggested_clusters)\n",
    "\n",
    "    \n",
    "# TODO can refine clusters for diffeent data sets\n",
    "# suggested_no_of_clusters(data_train, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(df, data_train, data_pred, num_clusters=5, threshold=10):\n",
    "    # Select relevant columns for clustering\n",
    "    columns_to_cluster = data_train.columns\n",
    "\n",
    "    # Extract the subset of data for clustering from training DataFrame\n",
    "    data_for_clustering = data_train[columns_to_cluster]\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_for_clustering_standardized = scaler.fit_transform(data_for_clustering)\n",
    "\n",
    "    # Apply k-means clustering on training data\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(data_for_clustering_standardized)\n",
    "\n",
    "    # Get the cluster labels for training data\n",
    "    training_cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Assign cluster labels to the training DataFrame\n",
    "    data_train['ClusterLabel'] = training_cluster_labels\n",
    "\n",
    "    # Check the distribution of clusters in training data\n",
    "    training_cluster_counts = data_train['ClusterLabel'].value_counts()\n",
    "\n",
    "    # Calculate cluster means on training data\n",
    "    training_cluster_means = data_train.groupby('ClusterLabel').mean()\n",
    "\n",
    "    # Extract the subset of data for anomaly prediction from predicting DataFrame\n",
    "    data_for_prediction = data_pred[columns_to_cluster]\n",
    "\n",
    "    # Standardize the data for prediction using the same scaler as training\n",
    "    data_for_prediction_standardized = scaler.transform(data_for_prediction)\n",
    "\n",
    "    # Get the cluster labels for prediction data\n",
    "    prediction_cluster_labels = kmeans.predict(data_for_prediction_standardized)\n",
    "\n",
    "    # Assign cluster labels to the predicting DataFrame\n",
    "    data_pred['ClusterLabel'] = prediction_cluster_labels\n",
    "\n",
    "    # Check the distribution of clusters in predicting data\n",
    "    prediction_cluster_counts = data_pred['ClusterLabel'].value_counts()\n",
    "\n",
    "    # Detect anomalies based on cluster means\n",
    "    anomaly_rows = []\n",
    "    for index, row in data_pred.iterrows():\n",
    "        cluster_label = row['ClusterLabel']\n",
    "        features = row[columns_to_cluster]\n",
    "        cluster_mean = training_cluster_means.loc[cluster_label]\n",
    "        if any(abs(features - cluster_mean) > threshold):\n",
    "            anomaly_rows.append(index)\n",
    "\n",
    "    # detected anomalies\n",
    "    anomalies = data_pred.loc[anomaly_rows]\n",
    "    \n",
    "    df['Anomalies_KMeans'] = 1  # Initialize the column with -1 values\n",
    "    df.loc[anomalies.index, 'Anomalies_KMeans'] = -1  # Set the corresponding anomalies as 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ad95b",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iforest(df, data_train, data_pred, outliers_fraction=0.5):\n",
    "    # Train\n",
    "    ifo = IsolationForest(contamination = outliers_fraction)\n",
    "    ifo.fit(data_train)\n",
    "\n",
    "    # Predict\n",
    "    df['Anomalies_IF'] = pd.Series(ifo.predict(data_pred))\n",
    "    \n",
    "    # Fill NaN values (samples from the training set) in the column with 1\n",
    "    df['Anomalies_IF'].fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ba10e",
   "metadata": {},
   "source": [
    "# OSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocsvm(df, data_train, data_pred, outliers_fraction=0.9):\n",
    "    # Train\n",
    "    osvm = OneClassSVM(nu = outliers_fraction, kernel = 'sigmoid')\n",
    "    osvm.fit(data_train)\n",
    "\n",
    "    # Predict\n",
    "    df['Anomalies_OSVM'] = pd.Series(osvm.predict(data_pred))\n",
    "    \n",
    "    # Fill NaN values (samples from the training set) in the column with 1\n",
    "    df['Anomalies_OSVM'].fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48e78b",
   "metadata": {},
   "source": [
    "# Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb86b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_of(df, data_train, data_pred, outliers_fraction=0.3):\n",
    "    # Train\n",
    "    lof = LocalOutlierFactor(contamination=outliers_fraction)\n",
    "    lof.fit(data_train)\n",
    "\n",
    "    # Predict\n",
    "    df['Anomalies_LOF'] = pd.Series(lof.fit_predict(data_pred))\n",
    "    \n",
    "    # Fill NaN values (samples from the training set) in the column with 1\n",
    "    df['Anomalies_LOF'].fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359f12d",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c3dc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def autoencoders(df, train, test):\n",
    "    # Define the shape of the input data\n",
    "    input_dim = train.shape[1]\n",
    "\n",
    "    # Define the architecture of the autoencoder\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(32, activation='relu')(input_layer)  # Encoding layer\n",
    "    decoder = Dense(input_dim, activation='linear')(encoder)  # Decoding layer\n",
    "\n",
    "    # Create the autoencoder model\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(train, train, epochs=50, batch_size=32, validation_data=(test, test))\n",
    "\n",
    "    # Use the trained autoencoder to predict on the test dataset\n",
    "    reconstructed_data = autoencoder.predict(test)\n",
    "\n",
    "    # Calculate the mean squared error (MSE) between the original and reconstructed data\n",
    "    mse = np.mean(np.power(test - reconstructed_data, 2), axis=1)\n",
    "\n",
    "    # Define a threshold to determine outliers\n",
    "    threshold = np.mean(mse) + 3 * np.std(mse)  # Adjust the multiplier (3) as per your requirements\n",
    "\n",
    "    # Map outliers as -1 and inliers as 1\n",
    "    outliers = np.where(mse > threshold, -1, 1)\n",
    "\n",
    "    # Return anomalies\n",
    "    anomalies = test.copy()\n",
    "    anomalies['Anomalies_AE'] = outliers    \n",
    "    \n",
    "    # Add the anomaly column to the original data\n",
    "    df['Anomalies_AE'] = 1  # Initialize all values as 1 (normal)\n",
    "    df.iloc[test.index, -1] = anomalies['Anomalies_AE'].values  # Assign anomaly values to test data indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025298cb",
   "metadata": {},
   "source": [
    "# Deep Support Vector Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsvdd(df, train, test):\n",
    "    # Train the autoencoder\n",
    "    input_dim = train.shape[1]\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    encoder = Dense(32, activation='relu')(input_layer)  # Adjust the number of nodes in the encoder layer\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    autoencoder.fit(train, train, epochs=50, batch_size=32, shuffle=True, validation_data=(test, test))\n",
    "\n",
    "    # Obtain the encoded representations from the trained autoencoder\n",
    "    encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "    encoded_train_data = encoder_model.predict(train)\n",
    "    encoded_test_data = encoder_model.predict(test)\n",
    "\n",
    "    # Train the ODSVDDM using the encoded representations\n",
    "    deep_svdd = DeepSVDD()\n",
    "    deep_svdd.fit(encoded_train_data)\n",
    "\n",
    "    # Predict the anomaly scores for the test data\n",
    "    anomaly_scores = deep_svdd.decision_function(encoded_test_data)\n",
    "\n",
    "    # Calculate the threshold for anomaly detection using the anomaly scores\n",
    "    threshold = np.percentile(anomaly_scores, 97)  # Adjust the percentile as needed\n",
    "\n",
    "    # Classify data points as normal (inliers) or anomalous (outliers)\n",
    "    predictions = np.where(anomaly_scores > threshold, -1, 1)\n",
    "\n",
    "    # Add the anomaly column to the original data\n",
    "    df['Anomalies_DSVDD'] = 1  # Initialize all values as 1 (inliers)\n",
    "    df.iloc[test.index, -1] = predictions  # Assign anomaly values to test data indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad6e98",
   "metadata": {},
   "source": [
    "# MICE\n",
    "Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move up 2 directories\n",
    "data_directory = '../..' \n",
    "\n",
    "# Load the CSV files\n",
    "asthma_df = pd.read_csv(os.path.join(data_directory, 'Data\\Preprocessed', 'preprocessed_MICE_asthma.csv'))\n",
    "healthy_df = pd.read_csv(os.path.join(data_directory, 'Data\\Preprocessed', 'preprocessed_MICE_healthy.csv'))\n",
    "\n",
    "# Merged df\n",
    "merged_df = pd.concat([asthma_df, healthy_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9a011",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preprocess for models\n",
    "train, pred = preprocess_for_models(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e49c99d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run all models and store anomaly results in merged_df\n",
    "k_means(merged_df, train, pred)\n",
    "iforest(merged_df, train, pred)\n",
    "local_of(merged_df, train, pred)\n",
    "ocsvm(merged_df, train, pred)\n",
    "autoencoders(merged_df, train, pred) \n",
    "dsvdd(merged_df, train, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df[['Anomalies_KMeans', 'Anomalies_IF', 'Anomalies_OSVM', 'Anomalies_LOF', 'Anomalies_AE', 'Anomalies_DSVDD']])\n",
    "# Select the desired columns\n",
    "columns = ['Anomalies_KMeans', 'Anomalies_IF', 'Anomalies_OSVM', 'Anomalies_LOF', 'Anomalies_AE', 'Anomalies_DSVDD']\n",
    "\n",
    "# Iterate over each column and print the count of -1s\n",
    "for col in columns:\n",
    "    count_neg_ones = (merged_df[col] == -1).sum()\n",
    "    print(f\"Column '{col}': Count of -1s = {count_neg_ones}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dddf1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "print(\"K-Means\")\n",
    "cm = confusion_matrix(pred, anomalies_kmeans)\n",
    "display_confusion_matrix(cm)\n",
    "calculate_metrics(cm)\n",
    "\n",
    "print(\"iforst\")\n",
    "cm = confusion_matrix(pred, anomalies_kmeans)\n",
    "display_confusion_matrix(cm)\n",
    "calculate_metrics(cm)\n",
    "\n",
    "print(\"ocsvm\")\n",
    "cm = confusion_matrix(pred, anomalies_kmeans)\n",
    "display_confusion_matrix(cm)\n",
    "calculate_metrics(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e2c0cb",
   "metadata": {},
   "source": [
    "# TODO try feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b76729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
